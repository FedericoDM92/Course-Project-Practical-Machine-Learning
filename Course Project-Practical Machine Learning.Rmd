---
title: "Course Project-Practical Machine Learning"
author: "Federico Di Marco"
date: "2022-08-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
In this project data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants are used to predict the quality of the exercise.

Four models are trained: 
-> Decision Tree 
-> Random Forest 
-> Gradient Boosted Trees 
-> Support Vector Machine 

Prediction using a validation set randomly selected from the training csv data is done to calculate the accuracy and out of sample error rate. The best model is used it to predict 20 cases using the test csv set.

## Preparation
Loading libraries and data. Moreover, the seed is set to ensure the reprotusibility of the results.
```{r Preparation}
#Libraries
library(lattice)
library(ggplot2)
library(caret)
library(kernlab)
library(rattle)
library(corrplot)

#Data
traincsv <- read.csv("./data/pml-training.csv")
testcsv <- read.csv("./data/pml-testing.csv")

#Seed
set.seed(1234)
```

## Data Exploration and Cleaning
```{r DEC 1}
head(traincsv)
```
The training set contains columns of NAs that can be removed.
Moreover, the variable in the columns from 1 to 7 are irrelevant for the outcome and then they are removed.
```{r DEC 2}
traincsv <- traincsv[,colSums(is.na(traincsv) | traincsv == "") < 100]
traincsv <- traincsv[,-c(1:7)]
```

## Train of the models
First of all let's split the traincsv data in train and test data.
```{r TOM 1}
inTrain <- createDataPartition(y=traincsv$classe, p=0.7, list=FALSE)
train <- traincsv[inTrain,]
test <- traincsv[-inTrain,]
```

# Decision Tree
Training:
```{r TOM 2}
control <- trainControl(method="cv", number=3, verboseIter=F)
mod_DT <- train(classe~., data=train, method="rpart", trControl = control, tuneLength = 5)
fancyRpartPlot(mod_DT$finalModel)
```
Prediction:
```{r TOM 3}
pred_DT <- predict(mod_DT, test)
cmDT <- confusionMatrix(pred_DT, factor(test$classe))
print(paste('Decision Tree accuracy:',as.character(round(cmDT$overall[1],3))))
```

# Random Forest
Training:
```{r TOM 4}
mod_RF <- train(classe~., data=train, method="rf", trControl = control, tuneLength = 5)
```

Prediction:
```{r TOM 5}
pred_RF <- predict(mod_RF, test)
cmRF <- confusionMatrix(pred_RF, factor(test$classe))
print(paste('Random Forest accuracy:',as.character(round(cmRF$overall[1],3))))
```

# Gradient Boosted Trees
Training:
```{r TOM 6}
mod_GBM <- train(classe~., data=train, method="gbm", trControl = control, tuneLength = 5, verbose = FALSE)
```
Prediction:
```{r TOM 7}
pred_GBM <- predict(mod_GBM, test)
cmGBM <- confusionMatrix(pred_GBM, factor(test$classe))
print(paste('Gradient Boosted Trees accuracy:',as.character(round(cmGBM$overall[1],3))))
```

# Support Vector Machine
Training:
```{r TOM 8}
mod_SVM <- train(classe~., data=train, method="svmLinear", trControl = control, tuneLength = 5, verbose = FALSE)
```
Prediction:
```{r TOM 9}
pred_SVM <- predict(mod_SVM, test)
cmSVM <- confusionMatrix(pred_SVM, factor(test$classe))
print(paste('Support Vector Machine accuracy',as.character(round(cmSVM$overall[1],3))))
```

# Prediction of the test set
In the previous analysis the best model turned out to be the random forest model and this is used for the predition of the test set.
```{r PTS}
pred <- predict(mod_RF, testcsv)
print(pred)
```



